#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Aug 21 12:52:01 2019

@author: RaMy
"""

from global_dqn_agent import GlobalDQNAgent as dqn
from env_back import ENV
import numpy as np
import matplotlib.pyplot as plt
import argparse


def main(args):
    # Environment variables
    nb_mec = 3
    nb_vnfs = 2
    # MECs
    min_cpu = 50
    max_cpu = 100
    min_ram = 50
    max_ram = 100
    min_disk = 131072
    max_disk = 524288
    # Containers
    min_c_cpu = 1
    max_c_cpu = 4
    min_c_ram = 1
    max_c_ram = 4
    min_c_disk = 512
    max_c_disk = 4096

    # DQN_agent
    episodes = 100                        # Total episodes for the training
    batch_size = 16                        # Total used memory in memory replay mode
    max_env_steps = 100                    # Max steps per episode

    # Generate the MEC environment
    env = ENV(nb_mec, nb_vnfs, min_cpu, max_cpu, min_ram, max_ram, min_disk, max_disk, min_c_cpu, max_c_cpu, min_c_ram,
              max_c_ram, min_c_disk, max_c_disk)
    env.generate_mec()
    env.generate_vnfs()
    # env.save_topology("environment")

    # Computation of action/states size
    action_size = len(env.vnfs) * (len(env.mec) + 2 * 3)
    print('action_size: {}'.format(action_size))

    state_size = len(env.mec) * 3 + len(env.vnfs) * 2
    print('state_size: {}'.format(state_size))

    if args['observe'] is not None:
        # testing the trained model
        epsilon = 0.0001
        agent = dqn(state_size, action_size, epsilon=epsilon)
        # Loading the Model's weights generated by the selected model
        agent.load(args['train'])
        for e in range(int(args['observe'])):
            state = env.get_state(True)
            state = np.reshape(state, [1, state_size])
            env.view_infrastructure_('environment_exploitation.txt')
            action = agent.act(state)
            next_state, reward, done, _ = env.step(action)
            env.view_infrastructure_('environment_exploitation.txt', False)

    else:
        # Training the model
        # Instantiating the DQN_Agent
        agent = dqn(state_size, action_size)

        tab = {}
        for e in range(episodes):
            # TODO: maybe this is causing problems, to be verified "state = env.get_state(True)"
            state = env.get_state(True)
            state = np.reshape(state, [1, state_size])
            for time in range(max_env_steps):
                env.view_infrastructure_('environment_exploration.txt')
                action = agent.act(state)
                next_state, reward, done, _ = env.step(action)
                tab[e * time + time] = {"action": action, "reward": reward, "next_state": next_state}
                env.view_infrastructure_('environment_exploration.txt', False)
                reward = reward if not done else -10
                next_state = np.reshape(next_state, [1, state_size])
                agent.remember(state, action, reward, next_state, done)
                state = next_state
                if time >= 99 and args['train'] != "dqn" and args['train'] != "dqn_batch":
                    print("episode: {}/{}, score: {}, e: {:.2}"
                          .format(e, episodes, time, agent.epsilon))
                    agent.update_target_model()
                    break
                if len(agent.memory) > batch_size:
                    if args['train'] == "dqn":
                        agent.replay_dqn(batch_size)
                    elif args['train'] == "dqn_batch":
                        agent.replay_dqn_batch(batch_size)
                    elif args['train'] == "fix_dqn":
                        print('helooo')
                        agent.replay_fixed_target_dqn(batch_size)
                    elif args['train'] == "fix_dqn_batch":
                        agent.replay_fixed_target_dqn_batch(batch_size)
                    elif args['train'] == "double_dqn":
                        agent.replay_double_dqn(batch_size)
                    elif args['train'] == "double_dqn_batch":
                        agent.replay_double_dqn_batch(batch_size)

        # Saving the Model's weights generated by the selected model
        agent.save("model_{}".format(args['train']))

        # Plotting the reward/avg_reward
        if args['train'] is not None:
            rewards = agent.ave_reward_list
            plt.plot((np.arange(len(rewards)) + 1), rewards)
            plt.xlabel('Episodes')
            plt.ylabel('Average Reward')
            plt.title('Average Reward vs Episodes')
            plt.savefig('rewards_{}.png'.format(args['train']))
            plt.close()
            # Saving all sort of statistics
            with open("action_state_information_{}.txt".format(args['train']), "w") as w:
                w.write(str(tab))

            with open("detailed_action_selection_{}.txt".format(args['train']), "w") as w:
                w.write(str(agent.action))

            with open("detailed_q_values_{}.txt".format(args['train']), "w") as w:
                w.write(str(agent.predict))

            with open("loss_{}.txt".format(args['train']), "w") as w:
                w.write(str(agent.loss))


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Parsing the type of DRL/RL to be tested')
    parser.add_argument('-t','--train', help='Train DRL/RL', required=True)
    parser.add_argument('-o', '--observe', help='Observe a trained DRL/RL')
    args = vars(parser.parse_args())
    # print(args)
    main(args)
